# Exploration of Residual Neural Network
##### Yassin Bahid

### Summary:
We look into different The residual neural networks architechture as presented in Kaiming He's paper. We shall implement his architecture on a smaller scale using only 9 layers instead of 34. We will then see how different Wide Residual Networks mprove on the model's accuracy when it oe to teh CIFAR10 dataset.


### File Structure:
\item  Data: Contains data files
\item resnet.ipynb: Jupyter Notebook containing report.

#### Reference:
Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun: “Deep Residual Learning for Image Recognition”, 2015; [http://arxiv.org/abs/1512.03385 arXiv:1512.03385].

Sergey Zagoruyko, Nikos Komodakis: “Wide Residual Networks”, 2016; arXiv:1605.07146.

CIFAR10, https://www.cs.toronto.edu/~kriz/cifar.html.

Resnet9, https://www.kaggle.com/code/kmldas/cifar10-resnet-90-accuracy-less-than-5-min